{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7a0efd",
   "metadata": {},
   "source": [
    "MNIST CNN Classification for FPGA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad548a",
   "metadata": {},
   "source": [
    "Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a06675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edfeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e265719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root = './data/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset  = datasets.MNIST(root = './data/', train = False, transform = transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "batch_size   = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104b203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "     # Initialization\n",
    "    def __init__(self):\n",
    "        super (CNN, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 3, 24, 24))\n",
    "        self.mp1_out_np   = np.zeros((1, 3, 12, 12))\n",
    "        \n",
    "        self.conv2_out_np = np.zeros((1, 3, 8, 8))\n",
    "        self.mp2_out_np   = np.zeros((1, 3, 4, 4))\n",
    "        \n",
    "        self.fc_in_np     = np.zeros((1, 48))\n",
    "        self.fc_out_np    = np.zeros((1, 10))\n",
    "        \n",
    "        # 1st Convolution Layer\n",
    "        # Image Input Shape -> (28, 28, 1)\n",
    "        # Convolution Layer -> (24, 24, 3)\n",
    "        # Pooling Max Layer -> (12, 12, 3)\n",
    "        self.conv1        = nn.Conv2d(1, 3, kernel_size=5)\n",
    "        \n",
    "        # 2nd Convolution Layer\n",
    "        # Image Input Shape -> (12, 12, 3)\n",
    "        # Convolution Layer -> (8, 8, 3)\n",
    "        # pooling Max Layer -> (4, 4, 3)\n",
    "        self.conv2        = nn.Conv2d(3, 3, kernel_size=5)\n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.mp   = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        # Num of Weight = 480\n",
    "        self.fc_1 = nn.Linear(48, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        # Layer Integration\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        \n",
    "        # Flatten Layer\n",
    "        x = x.view(in_size, -1)\n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        \n",
    "        return F.log_softmax(x)\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb729a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters: 796\n"
     ]
    }
   ],
   "source": [
    "# Instantiation\n",
    "model = CNN()\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de57ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ouput of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        # Loss calibration\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Back propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "# Test\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # Output of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "          \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2eaef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295420\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.278053\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.299701\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.286473\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.309746\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.298488\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.285126\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.273592\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.284173\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.272025\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.263491\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.248076\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.223349\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.175521\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.135850\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.108043\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.061913\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.041884\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.780281\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.702042\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.501504\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.407772\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.146428\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.996242\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.007156\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.055727\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.804472\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.804673\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.810158\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.489980\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.653735\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.513707\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.542439\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.587759\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.667017\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.489741\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.484465\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.448711\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.664968\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.497998\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.397932\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.522964\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.617569\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.772131\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.587582\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.526589\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.410488\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.596388\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.359443\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.443730\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.476617\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.394421\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.415094\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.476078\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.394791\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.341564\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.375541\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.263069\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.328669\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.359641\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.258237\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.474472\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.153190\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.315498\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.259248\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.200915\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.344550\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.588986\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.570004\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.309170\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.438507\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.440664\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.522280\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.269837\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.319593\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.605396\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.552204\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.326286\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.416430\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.267626\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.188703\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.458654\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.310683\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.350485\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.307875\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.362339\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.258725\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.468920\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.223227\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.226275\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.158194\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.193537\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.201033\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.264467\n",
      "\n",
      "Test set: Average loss: 0.3007, Accuracy: 9102/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.376363\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.348502\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.424030\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.424021\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.358661\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.132834\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.277420\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.345590\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.304389\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.343465\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.199632\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.289249\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.357535\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.225916\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.272524\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.294662\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.377367\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.427130\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.337059\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.594388\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.317672\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.426590\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.232514\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.334232\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.455467\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.422583\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.347058\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.502338\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.251776\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.332357\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.299425\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.152922\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.327542\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.246564\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.169741\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.491680\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.157204\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.202516\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.451884\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.174552\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.152887\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.334855\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.440945\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.350796\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.164994\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.158242\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.227933\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.507864\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.309177\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.567216\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.105825\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.382167\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.209804\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.240254\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.273830\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.235822\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.161144\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.195803\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.227099\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.278485\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.416141\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.111047\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.240389\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.360829\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.264857\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.402846\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.215808\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.263874\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.175440\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.237466\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.093202\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.204565\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.365350\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.132304\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.333864\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.252848\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.393876\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.324897\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.199979\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.342432\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.099386\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.117433\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.407794\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.235204\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.137536\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.076886\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.211753\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.160988\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.270831\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.104264\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.320058\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.267784\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.332604\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.236742\n",
      "\n",
      "Test set: Average loss: 0.2280, Accuracy: 9280/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.412435\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.235527\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.173370\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.209940\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.293869\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.180712\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.142922\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.209711\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.363448\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.144755\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.252075\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.161885\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.231462\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.148870\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.270831\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.337934\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.415865\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.171658\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.209011\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.223184\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.191050\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.384461\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.309906\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.322132\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.202689\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.085021\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.250494\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.189461\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.377071\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.123723\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.313384\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.286114\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.333678\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.216275\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.226641\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.159055\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.329945\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.149400\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.070534\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.174291\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.113867\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.183260\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.185845\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.127762\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.257393\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.228841\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.159062\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.263840\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.141110\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.136293\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.089094\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.280245\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.173722\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.150377\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.162881\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.126008\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.083679\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.099165\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.175564\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.270532\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.161146\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.167261\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.232004\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.128213\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.212047\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.143210\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.205452\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.235520\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.073440\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.204177\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.354165\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.149132\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.210512\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.232050\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.144854\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.157429\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.239778\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.280123\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.319690\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.108603\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.220888\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.312096\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.309794\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.272036\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.179049\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.193533\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.250275\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.320658\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.287116\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.131507\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.174448\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.145370\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.169978\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.125402\n",
      "\n",
      "Test set: Average loss: 0.1835, Accuracy: 9439/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.180917\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.129411\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.168941\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.121274\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.127766\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.126249\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.141756\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.207568\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.107737\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.225009\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.321386\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.124354\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.133864\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.185131\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.089009\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.149855\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.395929\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.143915\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.327323\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.211870\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.311538\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.151075\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.083873\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.090690\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.133331\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.317251\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.278683\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.250249\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.124602\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.125880\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.246389\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.154255\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.157736\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.087235\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.492510\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.127466\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.288149\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.135354\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.145488\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.219126\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.251934\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.069164\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.249676\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.123972\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.220639\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.134175\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.214079\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.196530\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.393440\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.160721\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.179066\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.263363\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.137812\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.138299\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.163592\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.125802\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.052458\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.170542\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.209025\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.257926\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.164026\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.270483\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.435562\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.256819\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.067504\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.073955\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.312235\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.336225\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.363754\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.108805\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.200087\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.184242\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.041084\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.132669\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.196273\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.213218\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.075278\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.214628\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.155414\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.230850\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.368180\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.271688\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.207551\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.084078\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.281857\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.163771\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.183358\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.190105\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.078558\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.233176\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.107909\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.257235\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.115937\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.141235\n",
      "\n",
      "Test set: Average loss: 0.1642, Accuracy: 9470/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.085278\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.190526\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.146751\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.242337\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.274180\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.385993\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.160653\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.139695\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.150166\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.165073\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.256458\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.099830\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.298678\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.118851\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.078290\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.200987\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.120852\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.325186\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.116747\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.182251\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.236796\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.116041\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.136793\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.276668\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.308712\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.188879\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.063302\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.192500\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.103090\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.206814\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.108390\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.180441\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.226280\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.248688\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.035300\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.165871\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.143875\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.086821\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.283349\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.271292\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.182398\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.191636\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.104171\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.158506\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.063058\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.085796\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.237261\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.142463\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.079307\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.196847\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.209749\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.175727\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.366174\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.062016\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.122764\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.120152\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.248144\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.213193\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.162489\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.262450\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.212044\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.127700\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.246503\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.144094\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.110606\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.091698\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.157388\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.193422\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.071930\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.109318\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.250218\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.229466\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.073026\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.225507\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.126766\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.201855\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.351194\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.102720\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.139219\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.078883\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.238326\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.211898\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.092974\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.102362\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.115901\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.034390\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.088046\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.255718\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.226669\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.044566\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.066288\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.054990\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.099191\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.190148\n",
      "\n",
      "Test set: Average loss: 0.1546, Accuracy: 9496/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.411511\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.116362\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.085319\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.202373\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.223009\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.218595\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.120004\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.129851\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.080196\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.104980\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.224474\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.195788\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.127965\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.197791\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.203274\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.265179\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.191472\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.136543\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.262314\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.234169\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.111897\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.121043\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.146701\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.186741\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.081152\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.307113\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.111754\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.186470\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.228539\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.138745\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.068773\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.051082\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.253441\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.077197\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.116823\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.092970\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.046613\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.178715\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.093075\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.264460\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.127999\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.089995\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.260716\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.044290\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.309512\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.299995\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.092594\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.109632\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.145186\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.086753\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.140004\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.101674\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.107797\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.175779\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.287694\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.117196\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.226440\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.108862\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.178335\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.336641\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.071321\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.138309\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.231001\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.069957\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.238133\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.172541\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.267398\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.188633\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.154779\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.119622\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.170409\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.035370\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.093168\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.108586\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.169641\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.203440\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.214604\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.226474\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.224546\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.129871\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.164131\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.041070\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.253201\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.101650\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.145061\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.085419\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.052668\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.090731\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.205067\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.271200\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.088221\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.130391\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.117341\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.170864\n",
      "\n",
      "Test set: Average loss: 0.1439, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.139883\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.081529\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.062642\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.135115\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.144946\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.070192\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.127806\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.060382\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.037422\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.191058\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.350309\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.136857\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.176685\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.110415\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.034620\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.158461\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.107786\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.224193\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.117966\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.245432\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.210518\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.121412\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.163886\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.083414\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.115807\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.131794\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.131953\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.146778\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.222998\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.082970\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.110947\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.122444\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.264276\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.112979\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.092025\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.066424\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.085604\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.370796\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.178795\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.232679\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.187695\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.089764\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.069368\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.090726\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.161998\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.065930\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.206784\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.207873\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.158724\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.193314\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.102142\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.194286\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.071179\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.314668\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.056354\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.114933\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.127453\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.234400\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.115073\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.057601\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.200726\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.216055\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.134388\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.150669\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.126771\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.040527\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.053836\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.137003\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.135556\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.122111\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.167344\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.104266\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.166312\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.120060\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.291275\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.178902\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.321068\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.122503\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.083299\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.085259\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.078087\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.162051\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.195586\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.084399\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.270623\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.057986\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.063086\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.206650\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.105019\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.163601\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.277023\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.066435\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.098812\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.141109\n",
      "\n",
      "Test set: Average loss: 0.1242, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.124844\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.305643\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.059758\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.108319\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.039437\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.141862\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.047736\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.068952\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.049355\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.096080\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.127297\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.103238\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.154487\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.125928\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.051841\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.106642\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.091688\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.163489\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.065113\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.082777\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.080273\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.178391\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.159251\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.068711\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.113029\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.124012\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.199665\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.155627\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.212801\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.237110\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.104558\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.064354\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.099194\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.108214\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.202383\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.089989\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.243905\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.030499\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.120884\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.122152\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.083994\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.090950\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.236075\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.175855\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.048773\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.050768\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.078068\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.176488\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.052164\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.059275\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.195970\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.050959\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.343310\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.085908\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.146271\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.157251\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.115420\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.181789\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.130970\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.040216\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.340317\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.227545\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.053763\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.215586\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.102818\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.073187\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.186192\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.147309\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.149221\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.036274\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.436322\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.122791\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.107706\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.142317\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.214670\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.120295\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.142577\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.353102\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.109117\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.052075\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.262924\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.112330\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.216004\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.137768\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.267080\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.250992\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.068613\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.087507\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.064739\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.186147\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.190904\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.145225\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.034446\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.112003\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 9610/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.106783\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.244974\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.117226\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.071438\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.082764\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.323614\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.229910\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.060682\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.035749\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.105955\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.123572\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.083525\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.103958\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.087370\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.258117\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.264026\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.179814\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.115497\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.071777\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.063006\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.042016\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.194112\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.065059\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.237302\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.117963\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.058909\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.114118\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.069549\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.256000\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.078959\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.054596\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.252535\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.103420\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.162896\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.050791\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.069041\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.233997\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.188883\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.158923\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.141462\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.059567\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.055823\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.139734\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.092620\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.154157\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.123300\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.064267\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.110383\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.157975\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.154965\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.306957\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.138688\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.055075\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.082745\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.169113\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.242276\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.083818\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.261474\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.136630\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.207072\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.212673\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.300008\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.037826\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.200453\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.194004\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.107068\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.193903\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.117977\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.090885\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.235543\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.134847\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.051175\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.063826\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.092584\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.063818\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.047968\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.105973\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.120815\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.024014\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.061915\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.075442\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.180524\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.207616\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.264629\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.156402\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.119218\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.250955\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.199471\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.061208\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.093216\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.233829\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.139210\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.043978\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.115493\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 9608/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traning process\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590374d6",
   "metadata": {},
   "source": [
    "Save Trained Weight and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed43c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 796\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# torch.save(model, \"./cnn_mnist.pt\")\n",
    "torch.save(model.state_dict(), \"./cnn_mnist_state_dict.pt\")\n",
    "\n",
    "# Load model\n",
    "# model = torch.load(\"./cnn_mnist.pt\", weights_only=False)\n",
    "# model.eval()\n",
    "# print(model)\n",
    "model = CNN() \n",
    "model.load_state_dict(torch.load(\"./cnn_mnist_state_dict.pt\"))\n",
    "model.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9adc17",
   "metadata": {},
   "source": [
    "Testing using a bitmap image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a34bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img       = Image.open(\"./bmp/train_0.bmp\", \"r\")\n",
    "np_img    = np.array(img)\n",
    "pyplot.imshow(np_img, cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "np_img_re = np.reshape(np_img, (1,1,28,28))\n",
    "data      = Variable(torch.tensor((np_img_re / 255), dtype = torch.float32))\n",
    "\n",
    "output    = model(data)\n",
    "pred      = output.data.max(1, keepdim=True)[1]\n",
    "print('Predicted output: ' + ', '.join(map(str, pred.flatten().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe82547",
   "metadata": {},
   "source": [
    "Extracted Weight and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc7dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[-62, -49, -12,  39,  56],\n",
      "        [-54, -26, -11,  35,  58],\n",
      "        [-34, -36,  21,  31,  49],\n",
      "        [-43, -19,  38,  58,  15],\n",
      "        [-20,  19,  29,  70,   4]], dtype=torch.int32)\n",
      "tensor([[ 21,   1, -20, -32, -11],\n",
      "        [ 26,  18,  19, -11, -14],\n",
      "        [ 28,  49,  82,  35,   0],\n",
      "        [ 17,  43, 111,  97,  43],\n",
      "        [ -2,  29,  66,  62,  87]], dtype=torch.int32)\n",
      "tensor([[-21,   6,  -8,  -8, -36],\n",
      "        [-29, -39, -53, -47, -34],\n",
      "        [ -4, -34, -29,  -5, -22],\n",
      "        [ 28,  33,  48,  11,  29],\n",
      "        [ 18,  13,  23,  54,  50]], dtype=torch.int32)\n",
      "tensor([-45,   0,  18], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[194, 207, 244,  39,  56],\n",
      "        [202, 230, 245,  35,  58],\n",
      "        [222, 220,  21,  31,  49],\n",
      "        [213, 237,  38,  58,  15],\n",
      "        [236,  19,  29,  70,   4]], dtype=torch.int32)\n",
      "tensor([[ 21,   1, 236, 224, 245],\n",
      "        [ 26,  18,  19, 245, 242],\n",
      "        [ 28,  49,  82,  35,   0],\n",
      "        [ 17,  43, 111,  97,  43],\n",
      "        [254,  29,  66,  62,  87]], dtype=torch.int32)\n",
      "tensor([[235,   6, 248, 248, 220],\n",
      "        [227, 217, 203, 209, 222],\n",
      "        [252, 222, 227, 251, 234],\n",
      "        [ 28,  33,  48,  11,  29],\n",
      "        [ 18,  13,  23,  54,  50]], dtype=torch.int32)\n",
      "tensor([211,   0,  18], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "int_conv1_weight_1 = torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_2 = torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_3 = torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv1_bias     = torch.tensor((model.conv1.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print(\"Signed\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "# Converting to 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] < 0:\n",
    "            int_conv1_weight_1[i][j] += 256\n",
    "        if int_conv1_weight_2[i][j] < 0:\n",
    "            int_conv1_weight_2[i][j] += 256\n",
    "        if int_conv1_weight_3[i][j] < 0:\n",
    "            int_conv1_weight_3[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv1_bias[k] < 0:\n",
    "        int_conv1_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "np.savetxt('conv1_weight_1.mem', int_conv1_weight_1, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_2.mem', int_conv1_weight_2, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_3.mem', int_conv1_weight_3, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_bias.mem', int_conv1_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8b4acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[ -7, -22,   3, -15, -11],\n",
      "        [ 12, -24, -34, -27,   2],\n",
      "        [ 18, -11, -26,  -3,  -2],\n",
      "        [ -3,  -5,  19,   1,  -9],\n",
      "        [ -9, -10,   8,  26,   0]], dtype=torch.int32)\n",
      "tensor([[ 50,  30, -36, -29, -22],\n",
      "        [ 28,  27,  38,   6, -55],\n",
      "        [ 15,  21,  46,  49, -30],\n",
      "        [ 10,  26,  40,  26, -27],\n",
      "        [ 41,  16,   1,  -2, -14]], dtype=torch.int32)\n",
      "tensor([[  1,  11,  16,   0,   1],\n",
      "        [-33,  10,  16,  18,   0],\n",
      "        [-35, -23, -22, -18,  -7],\n",
      "        [-13, -19, -15, -22, -16],\n",
      "        [ -6, -15,  -3,  -4,  -2]], dtype=torch.int32) \n",
      "\n",
      "tensor([[-36, -15,   4,  24,  17],\n",
      "        [ -8,  -3,   8,  24,   4],\n",
      "        [ 15,  13,  21,  18,   3],\n",
      "        [  0,  38,   5,   5,  -7],\n",
      "        [ 16,  37,   2, -28, -21]], dtype=torch.int32)\n",
      "tensor([[-14, -24, -34, -12, -31],\n",
      "        [-25,  12,  31,  11,  16],\n",
      "        [  7,  64,  39,   8,   4],\n",
      "        [ 50,  54,   4, -22, -15],\n",
      "        [  7,  38, -22, -45, -13]], dtype=torch.int32)\n",
      "tensor([[  7,  -9,  -5, -19,  -7],\n",
      "        [ 15,  -2,  15,  -2,   9],\n",
      "        [ 21, -21,  -4,   3,  -1],\n",
      "        [ 10, -18,   1,  17,  -3],\n",
      "        [-28,  -4,  19,  13, -12]], dtype=torch.int32) \n",
      "\n",
      "tensor([[ -9,  -7,   1,  16,  -5],\n",
      "        [  0, -27, -39, -38, -44],\n",
      "        [-26, -41, -45, -40, -40],\n",
      "        [-46, -41, -54, -21, -17],\n",
      "        [-48, -44, -17, -15,  -7]], dtype=torch.int32)\n",
      "tensor([[-13,  22,   0, -17,  22],\n",
      "        [  5,  76,  69,  48,  28],\n",
      "        [-25,   6,  43,  64,  43],\n",
      "        [-33, -84, -53,  -4,  16],\n",
      "        [ 12,  25,   2, -16, -38]], dtype=torch.int32)\n",
      "tensor([[ 5, 17, 37, 32, 22],\n",
      "        [ 5, 26, 42,  9,  2],\n",
      "        [13, 14, 23, 26, 10],\n",
      "        [ 9, 33, 31, 16, -2],\n",
      "        [34, 31, 36, 17, 14]], dtype=torch.int32) \n",
      "\n",
      "tensor([-32,   0,  -9], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[249, 234,   3, 241, 245],\n",
      "        [ 12, 232, 222, 229,   2],\n",
      "        [ 18, 245, 230, 253, 254],\n",
      "        [253, 251,  19,   1, 247],\n",
      "        [247, 246,   8,  26,   0]], dtype=torch.int32)\n",
      "tensor([[ 50,  30, 220, 227, 234],\n",
      "        [ 28,  27,  38,   6, 201],\n",
      "        [ 15,  21,  46,  49, 226],\n",
      "        [ 10,  26,  40,  26, 229],\n",
      "        [ 41,  16,   1, 254, 242]], dtype=torch.int32)\n",
      "tensor([[  1,  11,  16,   0,   1],\n",
      "        [223,  10,  16,  18,   0],\n",
      "        [221, 233, 234, 238, 249],\n",
      "        [243, 237, 241, 234, 240],\n",
      "        [250, 241, 253, 252, 254]], dtype=torch.int32) \n",
      "\n",
      "tensor([[220, 241,   4,  24,  17],\n",
      "        [248, 253,   8,  24,   4],\n",
      "        [ 15,  13,  21,  18,   3],\n",
      "        [  0,  38,   5,   5, 249],\n",
      "        [ 16,  37,   2, 228, 235]], dtype=torch.int32)\n",
      "tensor([[242, 232, 222, 244, 225],\n",
      "        [231,  12,  31,  11,  16],\n",
      "        [  7,  64,  39,   8,   4],\n",
      "        [ 50,  54,   4, 234, 241],\n",
      "        [  7,  38, 234, 211, 243]], dtype=torch.int32)\n",
      "tensor([[  7, 247, 251, 237, 249],\n",
      "        [ 15, 254,  15, 254,   9],\n",
      "        [ 21, 235, 252,   3, 255],\n",
      "        [ 10, 238,   1,  17, 253],\n",
      "        [228, 252,  19,  13, 244]], dtype=torch.int32) \n",
      "\n",
      "tensor([[247, 249,   1,  16, 251],\n",
      "        [  0, 229, 217, 218, 212],\n",
      "        [230, 215, 211, 216, 216],\n",
      "        [210, 215, 202, 235, 239],\n",
      "        [208, 212, 239, 241, 249]], dtype=torch.int32)\n",
      "tensor([[243,  22,   0, 239,  22],\n",
      "        [  5,  76,  69,  48,  28],\n",
      "        [231,   6,  43,  64,  43],\n",
      "        [223, 172, 203, 252,  16],\n",
      "        [ 12,  25,   2, 240, 218]], dtype=torch.int32)\n",
      "tensor([[  5,  17,  37,  32,  22],\n",
      "        [  5,  26,  42,   9,   2],\n",
      "        [ 13,  14,  23,  26,  10],\n",
      "        [  9,  33,  31,  16, 254],\n",
      "        [ 34,  31,  36,  17,  14]], dtype=torch.int32) \n",
      "\n",
      "tensor([224,   0, 247], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "# print(np.shape(model.conv2.weight))\n",
    "int_conv2_weight_11 = torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_12 = torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_13 = torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_21 = torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_22 = torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_23 = torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_31 = torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_32 = torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_33 = torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_bias = torch.tensor((model.conv2.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print (\"Signed\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "# Converting to 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] < 0:\n",
    "            int_conv2_weight_11[i][j] += 256\n",
    "        if int_conv2_weight_12[i][j] < 0:\n",
    "            int_conv2_weight_12[i][j] += 256\n",
    "        if int_conv2_weight_13[i][j] < 0:\n",
    "            int_conv2_weight_13[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] < 0:\n",
    "            int_conv2_weight_21[i][j] += 256\n",
    "        if int_conv2_weight_22[i][j] < 0:\n",
    "            int_conv2_weight_22[i][j] += 256\n",
    "        if int_conv2_weight_23[i][j] < 0:\n",
    "            int_conv2_weight_23[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_31[i][j] < 0:\n",
    "            int_conv2_weight_31[i][j] += 256\n",
    "        if int_conv2_weight_32[i][j] < 0:\n",
    "            int_conv2_weight_32[i][j] += 256\n",
    "        if int_conv2_weight_33[i][j] < 0:\n",
    "            int_conv2_weight_33[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv2_bias[k] < 0:\n",
    "        int_conv2_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "np.savetxt('conv2_weight_11.mem', int_conv2_weight_11, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_12.mem', int_conv2_weight_12, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_13.mem', int_conv2_weight_13, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_21.mem', int_conv2_weight_21, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_22.mem', int_conv2_weight_22, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_23.mem', int_conv2_weight_23, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_31.mem', int_conv2_weight_31, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_32.mem', int_conv2_weight_32, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_33.mem', int_conv2_weight_33, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_bias.mem', int_conv2_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585b8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n",
      "tensor([[ 17,  21, -11,  31,   2, -13,  10,  27,  -2,  23,  17, -29,  16,  40,\n",
      "          -5, -58,  -6,  29,  -9,  -3,  12,  17, -46,  -4,  -6, -10, -25,  36,\n",
      "          -2,   0,  21,   7, -11,   0,  19,   0, -43, -16, -16,  20, -24, -22,\n",
      "         -21,   4,  12,   6, -22,  34],\n",
      "        [-11,   9,   2,  12,  15,   9, -16,  -7,   0,   0, -16,  10,   9, -16,\n",
      "           5,  27,  -4, -39,  79, -17,  -3,  -3,  44, -29,  11, -30,  57, -50,\n",
      "          17,  27, -15,  22,   9,  -2, -57, -48,   0, -20, -13, -35, -27, -27,\n",
      "         -10,  -5, -24, -15,  38,  15],\n",
      "        [ -7,  -2,   0,  26, -38, -13,  -6,  -3,  -8,   7,  -3, -60, -22, -10,\n",
      "          29,  -7,  -3, -36, -32, -20,  44,   0,   2,  18,  31,  32,  33,   4,\n",
      "          -7,   1, -26,  43,  46,  33,  14,  -8,  28,   0,   6, -38, -58,  12,\n",
      "          -2,  31,   9,  -6,  20,  61],\n",
      "        [ -2,  -1,  -6,  41, -59, -51, -10, -27,  -8, -20,  22,  10,  39,  -8,\n",
      "           7, -16,  14, -30, -13,   2, -17,   0,   2, -23,  -7, -14, -13,  -3,\n",
      "          21,  18,  19,  15,  54,  24,  -4,  -2,  18,   3,  14,  19,  24,  36,\n",
      "          36,   2,   2,   2,  12, -10],\n",
      "        [ 37,  -2,   3,   2,   1,  -8, -31, -19,  13,  33, -20,   4,   6,  -6,\n",
      "           8,  31, -28,  16,  -6,  55,  28,  -7, -25,  21,  11,   1,  43,  21,\n",
      "          -2,  -4, -21, -20,  -5, -39, -75, -66, -12,  13, -21, -37,  30,  38,\n",
      "           4,  19,   0, -30,   0, -12],\n",
      "        [-14, -19,  -6, -83,   0,   9,   6, -35,  -3,  13,   8,  46,  -9,  -5,\n",
      "         -16,   4, -15,  21,  17, -10, -34, -15, -21,   6,   0, -21, -24, -35,\n",
      "          12,  37,  27,  29, -13,  16,  17,  55,   8,  -8,  35,  49,  39,  14,\n",
      "           5,  -6,  44,   0,  -6, -26],\n",
      "        [ -1, -19, -33, -39, -12,  26,  -5,  11,   7,  29,  36,  34,  -3,   7,\n",
      "         -10, -13,  20,  21,  -7, -51,   2,   0,  12,  -2, -42,  22, -28,  19,\n",
      "         -29,   9,  31,  18, -34, -65, -26,  37, -66, -53,   6,  42, -34, -15,\n",
      "          24,  19,  23,  31,  10,   5],\n",
      "        [ 13,   8, -20,  -3,  34, -15, -13,  17, -30, -25, -19, -13,   2, -28,\n",
      "         -19,   2,  -4,   4,  -3,  59,  34,  25, -10,  -5,   8,  40,  66,  46,\n",
      "         -24, -45, -13, -61,  66,  20,  25, -20,  24,  16,   9,  20,  19, -29,\n",
      "         -39,   8, -53, -26,   8,   2],\n",
      "        [-14,  -5,  -1,  26,  12,  24,   2, -22,   0,  17,  39,  -9,   7,  24,\n",
      "          37,  22, -12,  -7, -25,  -5,  -3, -27,  14,  16,   4,  44, -46,   0,\n",
      "         -10, -16, -10,   7, -16,  12,  16,  20,  34,   2,   4,  21, -22,  -7,\n",
      "           1,  34,   3, -21, -19,  -3],\n",
      "        [-42,   0,   8,  46, -14,  31,  26,  30,  27,   0, -28, -25,   8,   3,\n",
      "         -50, -13,  21,  15,   5,   6, -19, -44,   5,  41, -10, -22,   1,  -5,\n",
      "          -3,  -7,  -7, -35, -38, -12,  23,  22, -14,  23,  -2, -18,  65,  32,\n",
      "          -7, -57,   0,  14, -28, -22]], dtype=torch.int32)\n",
      "torch.Size([10])\n",
      "tensor([-22,  33,   2, -10,   0,   8,  -8, -11, -12,   0], dtype=torch.int32)\n",
      "tensor([[ 17,  21, 245,  31,   2, 243,  10,  27, 254,  23,  17, 227,  16,  40,\n",
      "         251, 198, 250,  29, 247, 253,  12,  17, 210, 252, 250, 246, 231,  36,\n",
      "         254,   0,  21,   7, 245,   0,  19,   0, 213, 240, 240,  20, 232, 234,\n",
      "         235,   4,  12,   6, 234,  34],\n",
      "        [245,   9,   2,  12,  15,   9, 240, 249,   0,   0, 240,  10,   9, 240,\n",
      "           5,  27, 252, 217,  79, 239, 253, 253,  44, 227,  11, 226,  57, 206,\n",
      "          17,  27, 241,  22,   9, 254, 199, 208,   0, 236, 243, 221, 229, 229,\n",
      "         246, 251, 232, 241,  38,  15],\n",
      "        [249, 254,   0,  26, 218, 243, 250, 253, 248,   7, 253, 196, 234, 246,\n",
      "          29, 249, 253, 220, 224, 236,  44,   0,   2,  18,  31,  32,  33,   4,\n",
      "         249,   1, 230,  43,  46,  33,  14, 248,  28,   0,   6, 218, 198,  12,\n",
      "         254,  31,   9, 250,  20,  61],\n",
      "        [254, 255, 250,  41, 197, 205, 246, 229, 248, 236,  22,  10,  39, 248,\n",
      "           7, 240,  14, 226, 243,   2, 239,   0,   2, 233, 249, 242, 243, 253,\n",
      "          21,  18,  19,  15,  54,  24, 252, 254,  18,   3,  14,  19,  24,  36,\n",
      "          36,   2,   2,   2,  12, 246],\n",
      "        [ 37, 254,   3,   2,   1, 248, 225, 237,  13,  33, 236,   4,   6, 250,\n",
      "           8,  31, 228,  16, 250,  55,  28, 249, 231,  21,  11,   1,  43,  21,\n",
      "         254, 252, 235, 236, 251, 217, 181, 190, 244,  13, 235, 219,  30,  38,\n",
      "           4,  19,   0, 226,   0, 244],\n",
      "        [242, 237, 250, 173,   0,   9,   6, 221, 253,  13,   8,  46, 247, 251,\n",
      "         240,   4, 241,  21,  17, 246, 222, 241, 235,   6,   0, 235, 232, 221,\n",
      "          12,  37,  27,  29, 243,  16,  17,  55,   8, 248,  35,  49,  39,  14,\n",
      "           5, 250,  44,   0, 250, 230],\n",
      "        [255, 237, 223, 217, 244,  26, 251,  11,   7,  29,  36,  34, 253,   7,\n",
      "         246, 243,  20,  21, 249, 205,   2,   0,  12, 254, 214,  22, 228,  19,\n",
      "         227,   9,  31,  18, 222, 191, 230,  37, 190, 203,   6,  42, 222, 241,\n",
      "          24,  19,  23,  31,  10,   5],\n",
      "        [ 13,   8, 236, 253,  34, 241, 243,  17, 226, 231, 237, 243,   2, 228,\n",
      "         237,   2, 252,   4, 253,  59,  34,  25, 246, 251,   8,  40,  66,  46,\n",
      "         232, 211, 243, 195,  66,  20,  25, 236,  24,  16,   9,  20,  19, 227,\n",
      "         217,   8, 203, 230,   8,   2],\n",
      "        [242, 251, 255,  26,  12,  24,   2, 234,   0,  17,  39, 247,   7,  24,\n",
      "          37,  22, 244, 249, 231, 251, 253, 229,  14,  16,   4,  44, 210,   0,\n",
      "         246, 240, 246,   7, 240,  12,  16,  20,  34,   2,   4,  21, 234, 249,\n",
      "           1,  34,   3, 235, 237, 253],\n",
      "        [214,   0,   8,  46, 242,  31,  26,  30,  27,   0, 228, 231,   8,   3,\n",
      "         206, 243,  21,  15,   5,   6, 237, 212,   5,  41, 246, 234,   1, 251,\n",
      "         253, 249, 249, 221, 218, 244,  23,  22, 242,  23, 254, 238,  65,  32,\n",
      "         249, 199,   0,  14, 228, 234]], dtype=torch.int32)\n",
      "tensor([234,  33,   2, 246,   0,   8, 248, 245, 244,   0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(model.fc_1.weight))\n",
    "print((model.fc_1.weight * 128).int())\n",
    "\n",
    "print(np.shape(model.fc_1.bias))\n",
    "print((model.fc_1.bias * 128).int())\n",
    "\n",
    "int_fc_weight = (model.fc_1.weight * 128).int()\n",
    "int_fc_bias = (model.fc_1.bias * 128).int()\n",
    "\n",
    "# Converting for 2's Complement\n",
    "for i in range(10):\n",
    "    for j in range(48):\n",
    "        if int_fc_weight[i][j] < 0 :\n",
    "            int_fc_weight[i][j] += 256\n",
    "    if int_fc_bias[i] < 0 :\n",
    "        int_fc_bias[i] += 256\n",
    "        \n",
    "print(int_fc_weight)\n",
    "print(int_fc_bias)\n",
    "\n",
    "np.savetxt('fc_weight.mem', int_fc_weight, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('fc_bias.mem', int_fc_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16344e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
